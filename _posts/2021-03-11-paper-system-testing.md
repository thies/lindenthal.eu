---
title: "Towards Accountability in Machine Learning
Applications: A System-testing Approach"
date: 2021-03-10
excerpt: "."
categories:
  - working papers
tags:
  - research
  - working papers
---

Wayne Wan and I drafted a new working paper &mdash; feedback would be very welcome.

A rapidly expanding universe of technology-focused startups is trying to change and improve the way real estate markets operate. The undisputed predictive power of machine learning (ML) models often plays a crucial role in the ‘disruption’ of traditional processes. However, an accountability gap prevails: How do the models arrive at their predictions? Do they do what we hope they do &mdash; or are corners cut?

Training ML models is a software development process at heart. We suggest following the
dedicated software testing framework and verifying that the ML model is performing as intended. Illustratively, we augment two image classifiers with a system testing procedure based on local interpretable model-agnostic explanation (LIME) techniques. Analyzing the classifications sheds light on some of the factors that determine the behavior of the systems. We show that cross-validation is simply not good enough when operating in regulated environments.

Full paper: [Link](/assets/paper/Wan and Lindenthal - System Testing.pdf)
